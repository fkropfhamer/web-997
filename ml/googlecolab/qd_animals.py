# -*- coding: utf-8 -*-
"""ml004_qd_animals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-JssMFbWgc7EIXUSoumvblfqiN_YrZSU
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

classes = ["ant", "bat", "bear", "bee", "bird", "butterfly", "camel", "cat", "cow", "crab", "crocodile", "dog", "dolphin", "duck", "elephant", "fish", "flamingo", "frog", "giraffe", "hedgehog", "horse", "kangaroo", "lion", "lobster", "monkey", "mosquito", "mouse", "octopus", "owl", "panda", "parrot", "penguin", "pig", "rabbit", "raccoon", "rhinoceros", "scorpion", "sea_turtle", "shark", "sheep", "snail", "snake", "spider", "squirrel", "swan", "tiger", "whale", "zebra"]
print(len(classes))

import glob
import urllib.request


allready_loaded_files = glob.glob('*.npy')
print(allready_loaded_files)
base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'

for c in classes:
  filename = c + '.npy'
  if filename not in allready_loaded_files:
    cls_url = c.replace('_', '%20')
    path = base+cls_url + '.npy'
    print(path)
    urllib.request.urlretrieve(path, filename)
  else: 
    print("skipping: ", filename)

import os
import glob
import numpy as np

all_files = glob.glob('*.npy')

features = np.empty([0, 784])
labels = np.empty([0])
class_names = []

print(all_files)

data_per_class = 10000

for idx, file in enumerate(all_files):
  data = np.load(file)
  data = data[0: data_per_class, :]
  labels_array = np.full(data.shape[0], idx)

  features = np.concatenate((features, data), axis=0)
  labels = np.append(labels, labels_array)

  class_name, _ = os.path.splitext(os.path.basename(file))
  class_names.append(class_name)

print(class_names)
num_classes = len(class_names)

import tensorflow.keras as keras
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

features, labels = shuffle(features, labels)

train_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=0, test_size=0.1)

train_features = train_features.reshape(train_features.shape[0], 28, 28, 1)
test_features = test_features.reshape(test_features.shape[0], 28, 28, 1)

train_features = train_features.astype('float32')
test_features = test_features.astype('float32')

train_features = train_features / 255.0
test_features = test_features / 255.0


train_labels = keras.utils.to_categorical(train_labels, num_classes)
test_labels = keras.utils.to_categorical(test_labels, num_classes)

print(train_features.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense

model = Sequential([
    Conv2D(32, (5,5), input_shape=(28, 28, 1), activation='relu'),
    Conv2D(32, (5, 5), activation='relu'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'),
    Dropout(0.25),
                             
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'),
    Conv2D(64, (5, 5), activation='relu'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax'),

])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Convolution2D

model = Sequential([
    Convolution2D(16, (3, 3), padding='same', input_shape=(28, 28, 1), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Convolution2D(32, (3, 3), padding='same', activation= 'relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Convolution2D(64, (3, 3), padding='same', activation= 'relu'),
    MaxPooling2D(pool_size =(2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(num_classes, activation='softmax'),
])

metrics=['top_k_categorical_accuracy']
# metrics=['accuracy']

model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=metrics)

model.fit(train_features, train_labels, validation_data=(test_features, test_labels), epochs=20, batch_size=64)

loss, accuracy = model.evaluate(test_features, test_labels)
print("loss: {}, accuracy: {}".format(loss, accuracy))

!mkdir model

model.save("model/model.h5")

with open("model/classes.txt", mode="w") as f:
  for class_name in class_names:
    f.write(class_name)
    f.write("\n")